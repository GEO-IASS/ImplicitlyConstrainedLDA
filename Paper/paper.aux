\relax 
\citation{Nigam2000}
\citation{Elworthy1994}
\citation{Weston2005}
\citation{Chapelle2006}
\citation{Cozman2006}
\citation{Loog2014a}
\citation{Krijthe2013}
\citation{McLachlan1975}
\citation{Taylor1977}
\citation{Yarowsky1995}
\citation{Abney2004}
\citation{Dempster1977}
\citation{Chapelle2006}
\citation{Zhu2003}
\citation{Bennett1998}
\citation{Joachims1999}
\citation{Grandvalet2005}
\citation{Fan2008}
\citation{Cai2007}
\citation{Loog2014a}
\citation{Krijthe2013}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{1}}
\citation{McLachlan1975}
\citation{Yarowsky1995}
\citation{Abney2004}
\citation{Dempster1977}
\citation{Nigam2000}
\citation{Loog2010}
\citation{Loog2014a}
\@writefile{toc}{\contentsline {section}{\numberline {III}Methods}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-A}}Supervised LDA}{2}}
\newlabel{eq:lda}{{1}{2}}
\newlabel{eq:ldasolution}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-B}}Self-Learning LDA (SLLDA)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-C}}Expectation Maximization LDA (EMLDA)}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-D}}Moment Constrained LDA (MCLDA)}{2}}
\newlabel{eq:constraintmean}{{5}{2}}
\citation{Loog2014a}
\citation{Loog2014a}
\citation{Loog2012b}
\citation{Bache2013}
\citation{Chapelle2006}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Bache2013}
\citation{Chapelle2006}
\newlabel{eq:constraintcovariance}{{6}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {III-E}}Implicitly Constrained LDA (ICLDA)}{3}}
\newlabel{eq:iclda}{{9}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experimental setup and results}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-A}}Toy problems}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\unhbox \voidb@x \hbox {IV-B}}Simulations on Benchmark datasets}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Behaviour on the two-class two dimensional gaussian datasets, with 10 labeled objects and 990 unlabeled objects. The first row shows the scatterplot and the trained responsibilities for respectively ICLDA and EMLDA on a dataset where the decision boundary does not adhere to the assumptions of EMLDA. The second row shows the results when the decision boundary is in between the two Gaussian classes. The black line indicates the decision boundary of a supervised learner trained using only the labeled data. Note that in the first row, the responsibilities of EM are very different from the true labels, while IC is not as sensitive to this problem.}}{4}}
\newlabel{fig:toyplots}{{1}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Semi-supervised learning curve on the Gaussian data set using 500 repeats. The shaded regions indicate one standard error around the mean. Since their assumptions hold exactly, SLLDA and EMLDA work very well. ICLDA also outperforms the supervised LDA.}}{4}}
\newlabel{fig:learningcurvegauss}{{2}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Description of the datasets used in the experiments}}{4}}
\newlabel{table:datasets}{{I}{4}}
\citation{Krijthe2013}
\citation{Bartlett2006}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Average 10-fold cross-validation error and its standard deviation over 20 repeats. Indicated in $\mathbf  {bold}$ is whether a semi-supervised classifier significantly outperform the supervised LDA classifier, as measured using a $t$-test with a $0.05$ significance level. \relax $\@@underline {\hbox {Underlined}}\mathsurround \z@ $\relax  indicates whether a semi-supervised classifier is (significantly) best among the four semi-supervised classifiers considered.}}{5}}
\newlabel{table:cvresults-error}{{II}{5}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Average 10-fold cross-validation negative log-likelihood (loss) and its standard deviation over 20 repeats. Indicated in $\mathbf  {bold}$ is whether a semi-supervised classifier significantly outperform the supervised LDA classifier, as measured using a $t$-test with a $0.05$ significance level. \relax $\@@underline {\hbox {Underlined}}\mathsurround \z@ $\relax  indicates whether a semi-supervised classifier is (significantly) best among the four semi-supervised classifiers considered.}}{5}}
\newlabel{table:cvresults-loss}{{III}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Learning curves for increasing amounts of unlabeled data for the error rate as well as the loss (negative log likelihood) for three datasets using 500 repeats. The shaded regions indicate one standard error around the mean.}}{5}}
\newlabel{fig:errorcurves}{{3}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Discussion}{5}}
\citation{Lehmann1998}
\citation{McLachlan1975}
\citation{Loog2014b}
\bibstyle{IEEEtranS}
\bibdata{IEEEabrv,library}
\bibcite{Nigam2000}{1}
\bibcite{Elworthy1994}{2}
\bibcite{Weston2005}{3}
\bibcite{Chapelle2006}{4}
\bibcite{Cozman2006}{5}
\bibcite{Loog2014a}{6}
\bibcite{Krijthe2013}{7}
\bibcite{McLachlan1975}{8}
\bibcite{Taylor1977}{9}
\bibcite{Yarowsky1995}{10}
\bibcite{Abney2004}{11}
\bibcite{Dempster1977}{12}
\bibcite{Zhu2003}{13}
\bibcite{Bennett1998}{14}
\bibcite{Joachims1999}{15}
\bibcite{Grandvalet2005}{16}
\bibcite{Fan2008}{17}
\bibcite{Cai2007}{18}
\bibcite{Loog2010}{19}
\bibcite{Loog2012b}{20}
\bibcite{Bache2013}{21}
\bibcite{Bartlett2006}{22}
\bibcite{Lehmann1998}{23}
\bibcite{Loog2014b}{24}
\@writefile{toc}{\contentsline {section}{\numberline {VI}Conclusion}{6}}
\@writefile{toc}{\contentsline {section}{References}{6}}
